>万物皆可线性

参考：西瓜书第3章 线性模型

## 1. 基本形式

* feature/data：样本的属性，训练参数
* label/target：样本的标签，训练结果

给定由d个属性描述的示例$\mathbf{x} = (x_1; x_2;...; x_d)$（通常情况下在数据集中，一个样本的表示形式为$\mathbf{x} = (x_1, x_2,...,x_d)$，区别在于数组的方向），其中$x_i$是$\mathbf{x}$在第i个属性上的取值，线性模型（linear model）试图学得一个通过属性的线性组合来进行预测的函数，即

$$
f(\mathbb{x}) = w_1x_1 + w_2x_2 + ... + w_dx_d + b
$$

一般用向量形式写成

$$
f(\mathbb{x}) = \mathbb{w}^T\mathbb{x} + b 
$$

其中$\mathbb{w} = (w_1; w_2;...; w_d)$，$\mathbb{w}$和$b$学得之后，模型就得以确定。

## 2. 线性回归

给定数据集$D = \{(\mathbb{x}_1, y_1), (\mathbb{x}_2, y_2),..., (\mathbb{x}_m, y_m)\}$，其中$\mathbb{x}_i = (x_{i1};x_{i2};...; x_{id};)$，$y_i \in \mathbb{R}$，线性回归（linear regression）试图学得一个线性模型以尽可能准确地预测实值输出标记。

> 首先，从单属性出发考虑如何求解，即$D = \{(x_i, y_i)\}^m_{i=1}$，线性回归试图学得

$$
f(x_i) = wx_i + b, 使得f(x_i) \simeq y_i
$$

我们使用均方误差MSE衡量$f(x)$与$y$之间的差异，并使得均方误差最小化，即

$$
(w^*, b^*) = \underset{(w, b)}{arg min}\sum^{m}_{i=1}(f(x_i) - y_i)^2
$$

$$
 = \underset{(w, b)}{arg min}\sum^{m}_{i=1}(y_i - wx_i - b)^2
$$

基于均方误差最小化来进行模型求解的方法称为“最小二乘法”，即找到一条直线使得所有样本到直线的欧式距离之和最小。

求解$w$和$b$使$E_{(w, b)} = \sum^m_{i=1}(y_i - wx_i - b)^2$最小化的过程，称为线性回归模型的最小二乘“参数估计”。将$E_{(w, b)}$分别对$w$和$b$求导，得到

$$
\frac{\partial E_{(w, b)}}{\partial w} = 2(w\sum_{i=1}^{m}x^2_i - \sum_{i=1}^{m}(y_i - b)x_i)
$$

$$
\frac{\partial E_{(w, b)}}{\partial b} = 2(mb - \sum_{i=1}^{m}(y_i - wx_i))
$$

通常对凸函数$E_{(w, b)}$来说，偏导数取值为零处即为最优解，因此$w$和$b$的最优闭式解

$$
w = \frac{\sum_{i=1}^{m}y_i(x_i - \bar{x})}{\sum_{i=1}^{m}x_i^2 - \frac{1}{m}(\sum_{i=1}^{m}x_i)^2}
$$

$$
b = \frac{1}{m}\sum_{i=1}^{m}(y_i - wx_i)
$$

其中$\bar{x} = \frac{1}{m}\sum_{i=1}^{m}x_i$为$x$的均值。

> 一般情况为样本包含d个属性，此时我们试图学得

$$
f(\mathbb{x}_i) = \mathbb{w}^T\mathbb{x}_i + b, 使得f(\mathbb{x}_i \simeq y_i)
$$

这称为“多元线性回归”.

同样利用最小二乘法对$\mathbb{w}$和$b$进行估计，这里令$\hat{\mathbb{w}} = (\mathbb{w}; b)$，相应的，数据集$D$表示为一个$m \times (d + 1)$的矩阵$\mathbb{X}$，其中每一行对应一个示例，即

$$
\mathbb{X} = 
\begin{pmatrix}
 x_{11}& x_{12} & ... & x_{1d} & 1\\ 
 x_{21}& x_{22} & ... & x_{2d} & 1\\ 
 \vdots& \vdots & \ddots & \vdots & \vdots\\ 
 x_{m1}& x_{m2} & ... & x_{md} & 1
\end{pmatrix}
= 
\begin{pmatrix}
 \mathbb{x}^T_1& 1\\ 
 \mathbb{x}^T_2& 1\\ 
 \vdots& \vdots\\ 
 \mathbb{x}^T_m& 1
\end{pmatrix}
$$

把标记也写成向量形式$\mathbb{y} = (y_1; y_2;...; y_m)$，令$E_{\hat{\mathbb{w}}} = (\mathbb{y} - \mathbb{X}\hat{\mathbb{w}})^T(\mathbb{y} - \mathbb{X}\hat{\mathbb{w}})$

$$
\hat{\mathbb{w}}^* = \underset{\hat{\mathbb{w}}}{argmin}E_{ehat{\mathbb{w}}}
$$

对$\hat{\mathbb{w}}$求导得到

$$
\frac{\partial E_{\hat{\mathbb{w}}}}{\partial \hat{\mathbb{w}}} = 2\mathbb{X}^T(\mathbb{X}\hat{\mathbb{w}} - y)
$$

同理，上式为零得到最优解，但是由于求解过程中使用到逆矩阵运算，所以会有分类讨论

1. 当$\mathbb{X}^T\mathbb{X}$为满秩矩阵或正定矩阵

$$
\hat{\mathbb{w}}^* = (\mathbb{X}^T\mathbb{X})^{-1}\mathbb{X}^T\mathbb{y}
$$

代入到$f(\mathbb{x})$可得最终模型为

$$
f(\hat{\mathbb{x}}_i) = \hat{\mathbb{x}}^T_i(\mathbb{X}^T\mathbb{X})^{-1}\mathbb{X}^T\mathbb{y}
$$

2. 当$\mathbb{X}^T\mathbb{X}$不是满秩矩阵，例如属性数量超过样例数，则会求解出多个$\hat{\mathbb{w}}$，选择哪一个将由学习算法的归纳偏好决定，常见的做法是引入正则化。

## 3. 对数几率回归



## 4. 线性判别分析LDA



## 5. 多分类学习



## 6. 类别不平衡问题



## 7. 